{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX6TEyNxBqFUqJt9Na4yWY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunlight2018/hands_on_ml3_notebooks/blob/main/chapter_02_end_to_end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æµç¨‹å›¾"
      ],
      "metadata": {
        "id": "tRcj5FPffoU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. è·å–æ•°æ®        ğŸ‘‰ fetch_housing_data()\n",
        "2. æ¢ç´¢æ•°æ®        ğŸ‘‰ .head(), .info(), .describe(), .hist()\n",
        "3. åˆ›å»ºæµ‹è¯•é›†      ğŸ‘‰ train_test_split() + StratifiedShuffleSplit\n",
        "4. æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç† ğŸ‘‰ dropna(), SimpleImputer, Pipeline\n",
        "5. ç‰¹å¾å·¥ç¨‹        ğŸ‘‰ ColumnTransformer + OneHotEncoder\n",
        "6. è®­ç»ƒæ¨¡å‹        ğŸ‘‰ fit() + predict()\n",
        "7. æ¨¡å‹è¯„ä¼°        ğŸ‘‰ RMSE, cross_val_score, æµ‹è¯•é›†éªŒè¯"
      ],
      "metadata": {
        "id": "fmQynh7Xfr3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å…³é”®è¯å¡"
      ],
      "metadata": {
        "id": "rTbRO0Tvf5U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¦‚å¿µ/æ¨¡å—\n",
        "è¯´æ˜\n",
        "\n",
        "StratifiedShuffleSplit\n",
        "åˆ†å±‚æŠ½æ ·ï¼Œç¡®ä¿è®­ç»ƒ/æµ‹è¯•åˆ†å¸ƒä¸€è‡´\n",
        "\n",
        "SimpleImputer\n",
        "ç¼ºå¤±å€¼å¡«è¡¥ï¼ˆå‡å€¼/ä¸­ä½æ•°/ä¼—æ•°ï¼‰\n",
        "\n",
        "Pipeline\n",
        "å°è£…æ•°æ®å¤„ç†æ­¥éª¤\n",
        "\n",
        "ColumnTransformer\n",
        "æ•°å€¼/ç±»åˆ«åˆ†å¼€å¤„ç†å†åˆå¹¶\n",
        "\n",
        "cross_val_score\n",
        "äº¤å‰éªŒè¯ï¼ˆæ›´ç¨³å®šçš„è¯„ä¼°æ–¹å¼ï¼‰\n",
        "\n",
        "fit_transform()\n",
        "å¸¸ç”¨äºæ•°æ®å¤„ç†é“¾æ¡ä¸­çš„ä¸€ç«™å¼è½¬æ¢\n",
        "\n",
        "LinearRegression / RandomForestRegressor\n",
        "æ¨¡å‹è®­ç»ƒå™¨\n"
      ],
      "metadata": {
        "id": "DTAHirHUgLmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  Hands-On ML ç¬¬ä¸‰ç‰ˆï¼šç¬¬äºŒç«  Colab å­¦ä¹ æ¨¡ç‰ˆ\n",
        "\n",
        "# âœ… 0. ç¯å¢ƒå‡†å¤‡\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# âœ… 1. ä¸‹è½½å’Œè§£å‹æ•°æ®\n",
        "def fetch_housing_data():\n",
        "    DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml3/main/\"\n",
        "    HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "    HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "    os.makedirs(HOUSING_PATH, exist_ok=True)\n",
        "    tgz_path = os.path.join(HOUSING_PATH, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(HOUSING_URL, tgz_path)\n",
        "    with tarfile.open(tgz_path) as housing_tgz:\n",
        "        housing_tgz.extractall(path=HOUSING_PATH)\n",
        "\n",
        "# âœ… 2. åŠ è½½æ•°æ®\n",
        "def load_housing_data():\n",
        "    csv_path = os.path.join(\"datasets\", \"housing\", \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "fetch_housing_data()\n",
        "housing = load_housing_data()\n",
        "housing.head()\n",
        "\n",
        "# âœ… 3. æ•°æ®åˆæ¢\n",
        "print(housing.info())\n",
        "print(housing[\"ocean_proximity\"].value_counts())\n",
        "print(housing.describe())\n",
        "housing.hist(bins=50, figsize=(20, 15))\n",
        "\n",
        "# âœ… 4. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                                bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                                labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]\n",
        "\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
        "\n",
        "# âœ… 5. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n",
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
        "\n",
        "# âœ… 6. æ•°å€¼åˆ—é¢„å¤„ç†ï¼šç¼ºå¤±å€¼å¡«è¡¥ + æ ‡å‡†åŒ–\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "housing_num = housing.select_dtypes(include=[np.number])\n",
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"std_scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
        "\n",
        "# âœ… 7. æ•°å€¼+ç±»åˆ«åˆ—é¢„å¤„ç†\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "num_attribs = list(housing_num.columns)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)\n",
        "\n",
        "# âœ… 8. è®­ç»ƒæ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "# âœ… 9. æ¨¡å‹è¯„ä¼°ï¼ˆRMSEï¼‰\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "print(\"çº¿æ€§å›å½’ RMSE:\", lin_rmse)\n",
        "\n",
        "# âœ… 10. æ›´å¤æ‚çš„æ¨¡å‹ï¼ˆéšæœºæ£®æ—ï¼‰\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "# âœ… 11. äº¤å‰éªŒè¯è¯„ä¼°\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "print(\"éšæœºæ£®æ—äº¤å‰éªŒè¯ RMSE å¹³å‡:\", rmse_scores.mean())\n",
        "\n",
        "# âœ… 12. æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = forest_reg.predict(X_test_prepared)\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "print(\"æœ€ç»ˆæµ‹è¯•é›† RMSE:\", final_rmse)\n",
        "\n",
        "# ğŸ¯ æœ¬ç« å®Œæˆï¼ä½ ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªç«¯åˆ°ç«¯ ML é¡¹ç›®çš„åŸºæœ¬æ¨¡æ¿ã€‚"
      ],
      "metadata": {
        "id": "znBSfA7MfaD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ9rt6lXe9yF"
      },
      "outputs": [],
      "source": [
        "æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºå¥½äº†ã€ŠHands-On Machine Learningã€‹ç¬¬ä¸‰ç‰ˆç¬¬ 2 ç« çš„ Colab å­¦ä¹ æ¨¡æ¿ï¼Œå†…å®¹åŒ…å«ï¼š\n",
        "\tâ€¢\tä»æ•°æ®è·å–åˆ°æµ‹è¯•é›†è¯„ä¼°çš„å®Œæ•´æµç¨‹ âœ…\n",
        "\tâ€¢\tç»“æ„æ¸…æ™°ã€æ¯æ­¥æ³¨é‡Šé½å…¨ âœ…\n",
        "\tâ€¢\tå¯ä»¥åå¤å¤ç”¨ï¼Œå¥—å…¥æ–°æ•°æ®é›†å°±èƒ½åšé¡¹ç›® âœ…\n",
        "\n",
        "ä½ å¯ä»¥å¤åˆ¶è¿™ä»½æ¨¡æ¿åˆ° Colab ä¸­ç›´æ¥ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥æŒ‰æ¨¡å—æ·»åŠ ç¬”è®°å’Œç»ƒä¹ ã€‚\n",
        "\n",
        "ä¸‹ä¸€æ­¥æˆ‘ä»¬å¯ä»¥æ•´ç†ç¬¬ 3 ç« ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰ã€æˆ–å¸®ä½ æŠŠè¿™ç« è½¬æˆ Markdown ç¬”è®°ï¼Œæˆ–è€…æ·»åŠ â€œç»ƒä¹ é¢˜æ¨¡å—â€ã€‚ä½ æƒ³ç»§ç»­å“ªä¸ªï¼Ÿ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A9CeYLMefFoH"
      }
    }
  ]
}